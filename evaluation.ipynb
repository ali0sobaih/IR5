{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a5eef42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    ps = PorterStemmer()\n",
    "    stems = [ps.stem(token) for token in filtered_tokens]\n",
    "    return \" \".join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbe41c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# services/search_classes.py (Content in a Jupyter Notebook cell)\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from sentence_transformers import SentenceTransformer # Also needed for BertSearch\n",
    "from rank_bm25 import BM25Okapi # Needed for Bm25Search if not already imported\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Needed for TfIdfSearch if not already imported\n",
    "\n",
    "# Ensure preprocess function is defined in a preceding cell or imported if from a separate .py file\n",
    "# from services.preprocessing_service import preprocess\n",
    "\n",
    "class TfIdfSearch:\n",
    "    def __init__(self, data):\n",
    "        self.vec = data[\"vectorizer\"]\n",
    "        self.mat = data[\"matrix\"]\n",
    "        self.doc_ids = data[\"doc_ids\"]\n",
    "\n",
    "    def execute_search(self, query):\n",
    "        # preprocess function must be available in the notebook's scope\n",
    "        vec = self.vec.transform([preprocess(query)])\n",
    "        scores = (self.mat @ vec.T).toarray().flatten()\n",
    "        top_idx = scores.argsort()[::-1][:10]\n",
    "        return [self.doc_ids[i] for i in top_idx]\n",
    "\n",
    "class Bm25Search:\n",
    "    def __init__(self, data):\n",
    "        self.bm25 = data[\"bm25\"]\n",
    "        self.doc_ids = data[\"doc_ids\"]\n",
    "        self.tokenized = data[\"tokenized_docs\"]\n",
    "\n",
    "    def execute_search(self, query):\n",
    "        # preprocess function must be available in the notebook's scope\n",
    "        tokens = preprocess(query).split()\n",
    "        scores = self.bm25.get_scores(tokens)\n",
    "        top_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:10]\n",
    "        return [self.doc_ids[i] for i in top_idx]\n",
    "\n",
    "class BertSearch:\n",
    "    def __init__(self, dataset):\n",
    "        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "        index_path = f\"faiss_store/{dataset}/index.faiss\"\n",
    "        self.index = faiss.read_index(index_path)\n",
    "\n",
    "        if self.index.ntotal > 0:\n",
    "            print(f\"FAISS index loaded for dataset: {dataset} (size: {self.index.ntotal})\")\n",
    "\n",
    "        conn = sqlite3.connect(\"offline/ir_project.db\")\n",
    "        cursor = conn.execute(f\"SELECT doc_id, doc FROM {dataset}\")\n",
    "        self.docs = {row[0]: row[1] for row in cursor}\n",
    "        self.doc_ids = list(self.docs.keys())\n",
    "        conn.close()\n",
    "\n",
    "    def execute_search(self, query):\n",
    "        # preprocess function must be available in the notebook's scope\n",
    "        processed = preprocess(query)\n",
    "        q_emb = self.model.encode([processed]).astype(\"float32\")\n",
    "        d, I = self.index.search(q_emb, 10)\n",
    "        top_ids = [self.doc_ids[i] for i in I[0] if i < len(self.doc_ids)]\n",
    "        return top_ids # Changed to return doc_ids as per evaluation needs\n",
    "\n",
    "class HybridSearch:\n",
    "    def __init__(self, dataset):\n",
    "        # Ensure get_search_service is defined in a preceding cell or accessible\n",
    "        from services.search_factory import get_search_service\n",
    "\n",
    "        self.bert = get_search_service(\"bert\", dataset)\n",
    "        self.bm25 = get_search_service(\"bm25\", dataset)\n",
    "        self.tfidf = get_search_service(\"tfidf\", dataset)\n",
    "\n",
    "    def execute_search(self, query):\n",
    "        bert_results = self.bert.execute_search(query)\n",
    "        bm25_results = self.bm25.execute_search(query)\n",
    "        tfidf_results = self.tfidf.execute_search(query)\n",
    "        combined = list(dict.fromkeys(bert_results + bm25_results + tfidf_results))[:10]\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e15ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_service(search_type, dataset):\n",
    "    if search_type == \"tfidf\":\n",
    "        return TfIdfSearch(dataset)\n",
    "    elif search_type == \"bm25\":\n",
    "        return Bm25Search(dataset)\n",
    "    elif search_type == \"bert\":\n",
    "        return BertSearch(dataset)\n",
    "    elif search_type == \"hybrid\":\n",
    "        return HybridSearch(dataset)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid search type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7222771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "def load_data(search_type, dataset):\n",
    "    if search_type == \"bert\" or search_type == \"hybrid\":\n",
    "        return dataset\n",
    "    joblib_data = joblib.load(f\"offline_data/{search_type}_{dataset}.joblib\")\n",
    "    joblib_data[\"dataset\"] = dataset \n",
    "    return joblib_data\n",
    "\n",
    "async def search(\n",
    "    query: str,\n",
    "    dataset: str,\n",
    "    search_type: str\n",
    "):\n",
    "    data = load_data(\"bert\" if search_type == \"bert\" else search_type, dataset)\n",
    "    service = get_search_service(search_type, data)\n",
    "    return service.execute_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e6707fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full evaluation...\n",
      "\n",
      "\n",
      "======== Evaluating Dataset: antique ========\n",
      "\n",
      "--- Search Type: TFIDF ---\n",
      "  Average MRR: 0.4110\n",
      "  Average Recall: 0.0653\n",
      "  Average MAP: 0.0463\n",
      "  Average P@1: 0.3250\n",
      "  Average P@5: 0.2260\n",
      "  Average P@10: 0.1910\n",
      "----------------------------------------\n",
      "\n",
      "--- Search Type: BERT ---\n",
      "FAISS index loaded for dataset: antique (size: 403666)\n",
      "  Average MRR: 0.5412\n",
      "  Average Recall: 0.0971\n",
      "  Average MAP: 0.0728\n",
      "  Average P@1: 0.4500\n",
      "  Average P@5: 0.3450\n",
      "  Average P@10: 0.2895\n",
      "----------------------------------------\n",
      "\n",
      "======== Evaluating Dataset: quora ========\n",
      "\n",
      "--- Search Type: TFIDF ---\n",
      "  Average MRR: 0.3018\n",
      "  Average Recall: 0.4144\n",
      "  Average MAP: 0.2787\n",
      "  Average P@1: 0.2385\n",
      "  Average P@5: 0.0889\n",
      "  Average P@10: 0.0541\n",
      "----------------------------------------\n",
      "\n",
      "--- Search Type: BERT ---\n",
      "FAISS index loaded for dataset: quora (size: 522931)\n",
      "  Average MRR: 0.5520\n",
      "  Average Recall: 0.6975\n",
      "  Average MAP: 0.5249\n",
      "  Average P@1: 0.4648\n",
      "  Average P@5: 0.1635\n",
      "  Average P@10: 0.0949\n",
      "----------------------------------------\n",
      "\n",
      "======== Full Evaluation Summary ========\n",
      "Dataset: antique\n",
      "  TFIDF: MRR: 0.4110, Recall: 0.0653, MAP: 0.0463, P@1: 0.3250, P@5: 0.2260, P@10: 0.1910\n",
      "  BERT: MRR: 0.5412, Recall: 0.0971, MAP: 0.0728, P@1: 0.4500, P@5: 0.3450, P@10: 0.2895\n",
      "------------------------------\n",
      "Dataset: quora\n",
      "  TFIDF: MRR: 0.3018, Recall: 0.4144, MAP: 0.2787, P@1: 0.2385, P@5: 0.0889, P@10: 0.0541\n",
      "  BERT: MRR: 0.5520, Recall: 0.6975, MAP: 0.5249, P@1: 0.4648, P@5: 0.1635, P@10: 0.0949\n",
      "------------------------------\n",
      "\n",
      "Full evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import joblib # Ensure joblib is imported if not already\n",
    "\n",
    "# --- Your load_data function (as provided by you) ---\n",
    "def load_data(search_type, dataset):\n",
    "    if search_type == \"bert\" or search_type == \"hybrid\":\n",
    "        return dataset # For BERT/Hybrid, the dataset itself is passed directly\n",
    "    joblib_data = joblib.load(f\"offline_data/{search_type}_{dataset}.joblib\")\n",
    "    joblib_data[\"dataset\"] = dataset\n",
    "    return joblib_data\n",
    "\n",
    "# --- Evaluation Data Loaders (rest of your functions) ---\n",
    "\n",
    "def load_queries_from_tsv(filepath):\n",
    "    \"\"\"Loads queries from a TSV file (query_id\\tquery_text).\"\"\"\n",
    "    queries = {}\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Warning: Query file not found at {filepath}\")\n",
    "        return queries\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t', 1)\n",
    "            if len(parts) == 2:\n",
    "                query_id, query_text = parts\n",
    "                queries[query_id] = query_text\n",
    "    return queries\n",
    "\n",
    "def load_qrels_from_tsv(filepath):\n",
    "    \"\"\"Loads qrels from a TSV file (query_id\\t0\\tdoc_id\\trelevance_score).\"\"\"\n",
    "    qrels = defaultdict(set) # Maps query_id to a set of relevant doc_ids\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Warning: Qrels file not found at {filepath}\")\n",
    "        return qrels\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        # Try to skip header if it exists (e.g., \"query-id\\tcorpus-id\\tscore\")\n",
    "        first_line_pos = f.tell() # Store current position\n",
    "        first_line = f.readline().strip()\n",
    "        # This line checks for common header patterns and skips the first line if it's a header\n",
    "        if not first_line.startswith(('query-id', 'query_id', '#')): \n",
    "            f.seek(first_line_pos) # Go back to the start if no header detected\n",
    "        \n",
    "        for line in f:\n",
    "            parts = line.strip().split() # This splits by any whitespace, accommodating both tab and space separators\n",
    "            if len(parts) == 4:\n",
    "                query_id, _, doc_id, relevance = parts # Unpacks 4 columns\n",
    "                try:\n",
    "                    if int(relevance) == 1: \n",
    "                        qrels[query_id].add(doc_id)\n",
    "                    elif int(relevance) == 2: \n",
    "                        qrels[query_id].add(doc_id)\n",
    "                        qrels[query_id].add(doc_id)\n",
    "                    elif int(relevance) == 3:\n",
    "                        qrels[query_id].add(doc_id)\n",
    "                        qrels[query_id].add(doc_id)\n",
    "                        qrels[query_id].add(doc_id)\n",
    "                    elif int(relevance) == 4: \n",
    "                        qrels[query_id].add(doc_id)\n",
    "                        qrels[query_id].add(doc_id)\n",
    "                        qrels[query_id].add(doc_id)\n",
    "                        qrels[query_id].add(doc_id)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            elif len(parts) == 3: # Handles 3-column format\n",
    "                query_id, doc_id, relevance = parts # Unpacks 3 columns\n",
    "                if int(relevance) == 1: # Consider any score >= 1 as relevant\n",
    "                        qrels[query_id].add(doc_id)\n",
    "            else:\n",
    "                continue # Skips any lines that don't match 3 or 4 columns\n",
    "\n",
    "            try:\n",
    "                if int(relevance) >= 1: # Consider any score >= 1 as relevant\n",
    "                    qrels[query_id].add(doc_id)\n",
    "            except ValueError:\n",
    "                continue # Skips lines where relevance score isn't an integer\n",
    "    return qrels\n",
    "\n",
    "def load_queries_from_jsonl_file(filepath):\n",
    "    \"\"\"Loads queries from a JSONL file (e.g., Quora format).\"\"\"\n",
    "    queries = {}\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Warning: Query JSONL file not found at {filepath}\")\n",
    "        return queries\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip(): # Skip empty lines\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    query_id = data.get(\"_id\")\n",
    "                    query_text = data.get(\"text\")\n",
    "                    if query_id and query_text:\n",
    "                        queries[query_id] = query_text\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Warning: Could not parse JSONL line from {filepath}: {line[:50]}...\")\n",
    "    return queries\n",
    "\n",
    "# --- Evaluation Metrics ---\n",
    "\n",
    "def calculate_precision_at_k(retrieved_docs, relevant_docs, k):\n",
    "    \"\"\"Calculates Precision@k.\"\"\"\n",
    "    if not relevant_docs or not retrieved_docs:\n",
    "        return 0.0\n",
    "    \n",
    "    # Ensure retrieved_docs are distinct and truncate to k\n",
    "    retrieved_docs_at_k = list(dict.fromkeys(retrieved_docs))[:k]\n",
    "    \n",
    "    hits = 0\n",
    "    for doc_id in retrieved_docs_at_k:\n",
    "        if doc_id in relevant_docs:\n",
    "            hits += 1\n",
    "    return hits / k if k > 0 else 0.0\n",
    "\n",
    "def calculate_mrr(retrieved_docs, relevant_docs):\n",
    "    \"\"\"Calculates Mean Reciprocal Rank (MRR) for a single query.\"\"\"\n",
    "    if not relevant_docs:\n",
    "        return 0.0\n",
    "    \n",
    "    for i, doc_id in enumerate(retrieved_docs):\n",
    "        if doc_id in relevant_docs:\n",
    "            return 1.0 / (i + 1)\n",
    "    return 0.0 # No relevant document found at all\n",
    "\n",
    "def calculate_recall(retrieved_docs, relevant_docs):\n",
    "    \"\"\"Calculates Recall.\"\"\"\n",
    "    if not relevant_docs:\n",
    "        return 0.0\n",
    "    \n",
    "    hits = 0\n",
    "    for doc_id in retrieved_docs:\n",
    "        if doc_id in relevant_docs:\n",
    "            hits += 1\n",
    "    return hits / len(relevant_docs)\n",
    "\n",
    "def calculate_average_precision(retrieved_docs, relevant_docs):\n",
    "    \"\"\"Calculates Average Precision (AP) for a single query.\"\"\"\n",
    "    if not relevant_docs or not retrieved_docs:\n",
    "        return 0.0\n",
    "\n",
    "    sum_precisions = 0.0\n",
    "    num_relevant_found = 0\n",
    "    for i, doc_id in enumerate(retrieved_docs):\n",
    "        if doc_id in relevant_docs:\n",
    "            num_relevant_found += 1\n",
    "            precision_at_i = num_relevant_found / (i + 1.0)\n",
    "            sum_precisions += precision_at_i\n",
    "    \n",
    "    return sum_precisions / len(relevant_docs) if len(relevant_docs) > 0 else 0.0\n",
    "\n",
    "\n",
    "# --- Main Evaluation Logic ---\n",
    "\n",
    "async def run_full_evaluation(datasets_info, search_types_to_evaluate, k_values=[1, 5, 10]):\n",
    "    \"\"\"\n",
    "    Runs a full evaluation across specified datasets and search types.\n",
    "    datasets_info: A dictionary mapping dataset names to their query/qrels file paths.\n",
    "                   e.g., {\"dataset_name\": {\"queries_path\": \"path/to/queries.txt\", \"qrels_path\": \"path/to/qrels.tsv\", \"query_format\": \"tsv\"}}\n",
    "    k_values: List of K values for Precision@K.\n",
    "    \"\"\"\n",
    "    overall_results = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n",
    "    \n",
    "    for dataset_name, dataset_config in datasets_info.items():\n",
    "        print(f\"\\n======== Evaluating Dataset: {dataset_name} ========\")\n",
    "        \n",
    "        queries = {}\n",
    "        if dataset_config.get(\"query_format\") == \"jsonl\":\n",
    "            queries = load_queries_from_jsonl_file(dataset_config[\"queries_path\"])\n",
    "        else: # Default to TSV if not specified or different\n",
    "            queries = load_queries_from_tsv(dataset_config[\"queries_path\"])\n",
    "\n",
    "        qrels = load_qrels_from_tsv(dataset_config[\"qrels_path\"])\n",
    "\n",
    "        if not queries:\n",
    "            print(f\"Skipping {dataset_name}: No queries loaded from {dataset_config['queries_path']}.\")\n",
    "            continue\n",
    "        if not qrels:\n",
    "            print(f\"Skipping {dataset_name}: No qrels loaded from {dataset_config['qrels_path']}.\")\n",
    "            continue\n",
    "        \n",
    "        for search_type in search_types_to_evaluate:\n",
    "            print(f\"\\n--- Search Type: {search_type.upper()} ---\")\n",
    "            \n",
    "            total_mrr = 0.0\n",
    "            total_recall = 0.0\n",
    "            total_map = 0.0\n",
    "            total_precision_at_k = defaultdict(float)\n",
    "            query_count_for_metrics = 0\n",
    "\n",
    "            try:\n",
    "                # Call load_data with both search_type and dataset_name\n",
    "                loaded_search_components = load_data(search_type, dataset_name)\n",
    "                # Ensure get_search_service is defined and returns valid search service objects\n",
    "                current_search_service = get_search_service(search_type, loaded_search_components) \n",
    "            except Exception as e:\n",
    "                print(f\"Error initializing search service for {search_type} on {dataset_name}: {e}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            for query_id, query_text in queries.items():\n",
    "                if query_id not in qrels:\n",
    "                    # print(f\"Warning: Query ID {query_id} not found in qrels. Skipping for metrics for this query.\")\n",
    "                    continue \n",
    "\n",
    "                relevant_docs_for_query = qrels[query_id]\n",
    "                \n",
    "                try:\n",
    "                    # --- CORRECTED: Removed 'await' here ---\n",
    "                    retrieved_docs = current_search_service.execute_search(query_text)\n",
    "                    \n",
    "                    mrr_score = calculate_mrr(retrieved_docs, relevant_docs_for_query)\n",
    "                    total_mrr += mrr_score\n",
    "\n",
    "                    recall_score = calculate_recall(retrieved_docs, relevant_docs_for_query)\n",
    "                    total_recall += recall_score\n",
    "\n",
    "                    ap_score = calculate_average_precision(retrieved_docs, relevant_docs_for_query)\n",
    "                    total_map += ap_score\n",
    "\n",
    "\n",
    "                    for k in k_values:\n",
    "                        pk_score = calculate_precision_at_k(retrieved_docs, relevant_docs_for_query, k)\n",
    "                        total_precision_at_k[k] += pk_score\n",
    "                    \n",
    "                    query_count_for_metrics += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during search for query '{query_id}' ({search_type}) in {dataset_name}: {e}\")\n",
    "            \n",
    "            if query_count_for_metrics > 0:\n",
    "                avg_mrr = total_mrr / query_count_for_metrics\n",
    "                print(f\"  Average MRR: {avg_mrr:.4f}\")\n",
    "                overall_results[dataset_name][search_type]['MRR'] = avg_mrr\n",
    "\n",
    "                avg_recall = total_recall / query_count_for_metrics\n",
    "                print(f\"  Average Recall: {avg_recall:.4f}\")\n",
    "                overall_results[dataset_name][search_type]['Recall'] = avg_recall\n",
    "\n",
    "                avg_map = total_map / query_count_for_metrics\n",
    "                print(f\"  Average MAP: {avg_map:.4f}\")\n",
    "                overall_results[dataset_name][search_type]['MAP'] = avg_map\n",
    "\n",
    "\n",
    "                for k in k_values:\n",
    "                    avg_pk = total_precision_at_k[k] / query_count_for_metrics\n",
    "                    print(f\"  Average P@{k}: {avg_pk:.4f}\")\n",
    "                    overall_results[dataset_name][search_type][f'P@{k}'] = avg_pk\n",
    "            else:\n",
    "                print(f\"  No queries with relevance judgments found for {dataset_name} ({search_type}) to calculate metrics.\")\n",
    "            \n",
    "            print(\"-\" * 40) # Separator for search types\n",
    "    \n",
    "    print(\"\\n======== Full Evaluation Summary ========\")\n",
    "    for dataset, search_types_results in overall_results.items():\n",
    "        print(f\"Dataset: {dataset}\")\n",
    "        for search_type, metrics in search_types_results.items():\n",
    "            metric_str = \", \".join([f\"{m}: {v:.4f}\" for m, v in metrics.items()])\n",
    "            print(f\"  {search_type.upper()}: {metric_str}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "# --- Configuration for your evaluation ---\n",
    "\n",
    "EVAL_DATASETS_INFO = {\n",
    "    \"antique\": { \n",
    "        \"queries_path\": \"data/antique/queries.txt\",\n",
    "        \"qrels_path\": \"data/antique/qrels.tsv\",\n",
    "        \"query_format\": \"tsv\"\n",
    "    },\n",
    "    \"quora\": { \n",
    "        \"queries_path\": \"data/quora/queries.jsonl\",\n",
    "        \"qrels_path\": \"data/quora/qrels/test.tsv\", \n",
    "        \"query_format\": \"jsonl\"\n",
    "    }\n",
    "}\n",
    "# --- CORRECTED: Removed 'bm25' and 'hybrid' as requested ---\n",
    "EVAL_SEARCH_TYPES = [\"tfidf\", \"bert\"]\n",
    "EVAL_K_VALUES = [1, 5, 10]\n",
    "\n",
    "# --- Execute the full evaluation ---\n",
    "if __name__ == \"__main__\":\n",
    "    # If running this in a single Jupyter cell and encountering issues with asyncio,\n",
    "    # you might need to uncomment and run the following lines once at the beginning of your notebook:\n",
    "    # import nest_asyncio\n",
    "    # nest_asyncio.apply()\n",
    "    \n",
    "    print(\"Starting full evaluation...\\n\")\n",
    "    # You still need to ensure that get_search_service and your search classes (TfIdfSearch, Bm25Search, etc.)\n",
    "    # are defined and accessible in your environment.\n",
    "    # Also, ensure 'offline_data' directory exists and contains your .joblib files\n",
    "    # if you are using 'tfidf' or 'bm25' search types.\n",
    "    await run_full_evaluation(EVAL_DATASETS_INFO, EVAL_SEARCH_TYPES, EVAL_K_VALUES)\n",
    "    print(\"\\nFull evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "940ebf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"data/quora/qrels/test.tsv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
